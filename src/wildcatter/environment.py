"""Driller environment module."""


from __future__ import annotations

import random
from typing import Any

import numpy as np
from gym import Env
from gym.spaces import Box
from gym.spaces import Discrete
from numpy.typing import NDArray


class SimpleDriller(Env):  # type: ignore
    """Simple driller environment."""

    def __init__(self, env_config: dict[str, Any]) -> None:
        """Initialize environment with config dictionary."""
        self.model = np.loadtxt(
            env_config["model_path"],
            delimiter=env_config["delim"],
        )

        self.nrow, self.ncol = self.model.shape
        self.available_pipe = env_config["available_pipe"]

        self.production = 0
        self.pipe_used = 0
        self.trajectory: list[list[int]] = []
        self.bit_location: list[int] = []

        self.action_space = Discrete(4)

        self.observation_space = Box(
            low=0, high=1, shape=(self.nrow, self.ncol), dtype="bool"
        )
        self.reset()

    def step(  # noqa: C901
        self, action: int
    ) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:
        """Take step based on action."""
        done = False
        actions = {
            0: [1, 0],  # down
            1: [0, -1],  # left
            2: [0, 1],  # right
            3: [-1, 0],  # up
        }

        dz_dx = actions[action]
        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]

        self.bit_location = new_location

        self.trajectory.append(new_location)
        newrow, newcol = new_location

        self.pipe_used += 1

        if newrow < 1 or newrow >= self.nrow:
            done = True
            reward = -100

        elif newcol < 0 or newcol >= self.ncol:
            done = True
            reward = -100

        else:
            reward = self.model[newrow, newcol] + self.pipe_used / 2
            self.update_state()

        if self.pipe_used == self.available_pipe:
            done = True
            reward = 0

        if self.bit_location in self.trajectory[:-1]:
            done = True
            reward = -100

        info: dict[str, Any] = {}

        return self.state, reward, done, info

    def update_state(self) -> None:
        """Update state method."""
        traj_i, traj_j = np.asarray(self.trajectory).T
        self.state[traj_i, traj_j] = 1

    def render(self) -> None:
        """Gym environment rendering."""
        raise NotImplementedError("No renderer implemented yet.")

    def reset(self) -> NDArray[np.bool_]:
        """Reset the status of the environment."""
        self.surface_hole_location = [1, random.randint(0, self.ncol - 1)]  # noqa: S311
        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)
        self.bit_location = self.surface_hole_location
        self.trajectory = [self.surface_hole_location]
        self.pipe_used = 0
        return self.state

"""otherclass"""  
class MultiDriller(Env):
    
    def __init__(self, env_config: dict[str, Any], nwells) -> None:
        """Initialize environment with config dictionary."""
        self.env_config = env_config 
        self.model = np.loadtxt(
            env_config["model_path"],
            delimiter=env_config["delim"],
        )

        self.nrow, self.ncol = self.model.shape
        self.nwells = nwells
        self.wells = self.createWells()
        self.reset()
        
    def createWells(self) -> list[SimpleDriller]:
        if self.nwells <= 1:
            raise ValueError('The number of wells should be more than 1')
        else:
            wells: list[SimpleDriller] = []
            for n in range(0, self.nwells):
                wells.append(SimpleDriller(self.env_config))
            return wells
        
    def getState(self, well: int) -> NDArray[np.bool_]:
        return self.wells[well].state
    
    def getTrajectory(self, well: int) -> list[list[int]]:
        return self.wells[well].trajectory
    
    def reset(self) -> None:
        for well in self.wells:
            well.reset()
    
    def step(self) -> NPArray[np.intc]:
        done = False
        w = 0
        score = np.zeros(self.nwells)
        for well in self.wells:
            done = False
            while not done:
                action = well.action_space.sample()
                state, reward, done, info = well.step(action)
                score[w] += reward
            w+=1
        return score
    
    def step(self, action: int) -> NPArray[np.intc]:
        done = False
        w = 0
        score = np.zeros(self.nwells)
        for well in self.wells:
            done = False
            while not done:
                state, reward, done, info = well.step(action)
                score[w] += reward
            w+=1
        return score